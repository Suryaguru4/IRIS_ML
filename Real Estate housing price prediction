# -*- coding: utf-8 -*-
"""Realestate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S__Wrzvx4M52B1PQJrYMNAkdUSYU-QjI
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as py
from google.colab import files
uploaded = files.upload()
import io

df = pd.read_csv(io.BytesIO(uploaded["Bengaluru_House_Data1.csv"]))
df.head()

df2 = df.drop(['area_type','availability','society','balcony'], axis='columns')
df2.head()

df2.isnull().sum()

df3 = df2.dropna()
df3.head()

df3.isnull().sum()

df3['size'].unique()

df3['new_size'] = df3['size'].apply(lambda x: int(x.split(' ')[0]))
df3.head()

def is_float(x):
  try:
    float(x)
  except:
    return False
  return True

df3[~df3['total_sqft'].apply(is_float)].head()

def is_removed(x):
  temp = x.split(" - ")
  if len(temp) == 2:
    return (float(temp[0])+float(temp[1]))/2
  try:
    return float(x)
  except:
    return None

df4 = df3.copy()
df4['total_sqft'] = df4['total_sqft'].apply(is_removed)
df4.head()

df4.isnull().sum()

df4 = df4.dropna()
df4.isnull().sum()

df5 = df4.copy()
df5['price_per_sqft'] = (df5['price']*100000) / df5['total_sqft']
df5.head()

df5.shape

unique_count = len(df5['location'].unique())
unique_count

df5['location'] = df5['location'].apply(lambda x: x.strip())
location_status = df5.groupby('location')['location'].agg('count').sort_values(ascending = False)
location_status

len(location_status[location_status<=10])

location_lessthan10 = location_status[location_status<=10]
location_lessthan10

df5.location = df5.location.apply(lambda x: 'other' if x in location_lessthan10 else x)
df5.location.unique()
df5.shape

df6 = df5[~(df5.total_sqft/df5.new_size<300)]
df6.shape

df6.	price_per_sqft.describe()

df7 = df6[~(df6.total_sqft/df5.new_size < 300)]
df7.shape

df7.head()

df7.price_per_sqft.describe()

def removeot (df):
  df_out = pd.DataFrame()
  for key, sub in df.groupby('location'):
    m = np.mean(sub.price_per_sqft)
    sd = np.std(sub.price_per_sqft)
    removed = sub[(sub.price_per_sqft>(m-sd)) & (sub.price_per_sqft <= (m+sd))]
    df_out = pd.concat([df_out, removed], ignore_index=True)
  return df_out

df8 = removeot(df7)
df8.head()

df8.price_per_sqft.describe()

py.hist(df8.price_per_sqft, rwidth = 0.8)
py.xlabel("price")
py.ylabel('count')

df9 = df8[df8.bath<df8.new_size+2]
df9.head()

df_dummies = pd.get_dummies(df9.location)
df_dummies.head()

df10 = pd.concat([df9, df_dummies.drop('other', axis='columns')],axis='columns')
df10.head()

df11 = df10.drop(['location','size','price_per_sqft'], axis = 'columns')

X = df11.drop('price',axis='columns')
X.head()

y = df11.price.astype(int)
y

from sklearn.model_selection import train_test_split
X_train,X_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state = 10)

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score

lr_clf = LinearRegression()
lr_clf.fit(X_train,y_train)
lr_clf.score(X_test,y_test)

cv = ShuffleSplit(n_splits=5,test_size=0.2,random_state=0)
cross_val_score(LinearRegression(), X, y,cv =cv)

def find_best_model_using_gridsearchcv(X,y):
    algos = {
        'linear_regression' : {
            'model': LinearRegression(),
            'params': {
                'normalize': [True, False]
            }
        },
        'lasso': {
            'model': Lasso(),
            'params': {
                'alpha': [1,2],
                'selection': ['random', 'cyclic']
            }
        },
        'decision_tree': {
            'model': DecisionTreeRegressor(),
            'params': {
                'criterion' : ['mse','friedman_mse'],
                'splitter': ['best','random']
            }
        }
    }
    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
    for algo_name, config in algos.items():
        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
        gs.fit(X,y)
        scores.append({
            'model': algo_name,
            'best_score': gs.best_score_,
            'best_params': gs.best_params_
        })

    return pd.DataFrame(scores,columns=['model','best_score','best_params'])

find_best_model_using_gridsearchcv(X,y)
